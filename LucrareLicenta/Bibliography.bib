
@manual{stft3,
	title  = {Fourier Transform},
	author = {P. J. Bavel},
	url    = {http://www.thefouriertransform.com/}
}
@manual{stft4,
	title  = {Transformata Fourier},
	author = {...},
	url    = {https://ro.wikipedia.org/wiki/Transformata_Fourier},
	year = {2018}
}
@book{stft1,
	author = {Smith, Julius},
	year = {2008},
	month = {01},
	pages = {},
	title = {Spectral Audio Signal Processing}
}
@manual{stft5,
	title  = {Short-time Fourier transform},
	author = {...},
	url    = {https://en.wikipedia.org/wiki/Short-time_Fourier_transform},
	year = {2018}
}
@manual{mfcc1,
	title  = {Mel Frequency Cepstral Coefficient (MFCC) tutorial},
	author = {James Lyons},
	url    = {http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs//#computing-the-mel-filterbank},
	year = {2013}
}
@manual{mfcc2,
	title  = {Speech Processing for Machine Learning: Filter banks, Mel-Frequency Cepstral Coefficients (MFCCs) and What's In-Between},
	author = {Haytham Fayek},
	url    = {https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html},
	year = {2016}
}
@manual{mfcc3,
	title  = {Mel-frequency cepstrum},
	author = {...},
	url    = {https://en.wikipedia.org/wiki/Mel-frequency_cepstrum},
	year = {2019}
}
@manual{mfcc4,
	title  = {Mel scale},
	author = {...},
	url    = {https://en.wikipedia.org/wiki/Mel_scale},
	year = {2020}
}
@manual{mfcc5,
	title  = {Periodogram},
	author = {...},
	url    = {https://en.wikipedia.org/wiki/Periodogram},
	year = {2019}
}
@manual{vad,
	title  = {Py-webrtcvad},
	author = {John Wiseman},
	url    = {https://github.com/wiseman/py-webrtcvad},
	year = {2019}
}
@manual{qdarkgraystyle,
	title  = {qdarkgraystyle},
	author = {Michell Stuttgart},
	url    = {https://github.com/mstuttgart/qdarkgraystyle},
	year = {2019}
}
@book{10.5555/1162264,
	author = {Bishop, Christopher M.},
	title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	year = {2006},
	isbn = {0387310738},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg}
}
@book{10.5555/2380985,
	author = {Murphy, Kevin P.},
	title = {Machine Learning: A Probabilistic Perspective},
	year = {2012},
	isbn = {0262018020},
	publisher = {The MIT Press}
}









@article{blanton,
	author = {Blanton, Smiley},
	year = {1915},
	month = {01},
	pages = {154-172},
	title = {The voice and the emotions},
	volume = {1},
	journal = {Quarterly Journal of Speech - QUART J SPEECH},
	doi = {10.1080/00335631509360475}
}
@article{leviholler,
	author = {Levinson, Stephen and Holler, Judith},
	year = {2014},
	month = {09},
	pages = {},
	title = {The origin of human multi-modal communication},
	volume = {369},
	journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
	doi = {10.1098/rstb.2013.0302}
}
@article{chriskirbi,
author = {Christiansen, Morten and Kirby, Simon},
year = {2003},
month = {08},
pages = {300-307},
title = {Language Evolution: Consensus and Controversies},
volume = {7},
journal = {Trends in cognitive sciences},
doi = {10.1016/S1364-6613(03)00136-0}
}
@article{dellaert,
author = {Dellaert, Frank and Polzin, Thomas and Waibel, Alex},
year = {1996},
month = {12},
pages = {},
title = {Recognizing Emotion In Speech},
volume = {3},
journal = {International Conference on Spoken Language Processing, ICSLP, Proceedings}
}
@inproceedings{huahu,
author = {Huahu, Xu and Jian, Yuan and Jue, Gao},
year = {2010},
month = {11},
pages = {537 - 541},
title = {Application of Speech Emotion Recognition in Intelligent Household Robot},
doi = {10.1109/AICI.2010.118}
}
@inproceedings{gupta,
	author = {Gupta, Purnima and Rajput, Nitendra},
	year = {2007},
	month = {01},
	pages = {2241-2244},
	title = {Two-stream emotion recognition for call center monitoring.}
}
@inbook{szwoch,
author = {Szwoch, Mariusz and Szwoch, Wioleta},
year = {2015},
month = {01},
pages = {227-236},
title = {Emotion Recognition for Affect Aware Video Games},
isbn = {978-3-319-10661-8},
doi = {10.1007/978-3-319-10662-5_28}
}
@article{lancker,
	author = {Van Lancker Sidtis, Diana and Cornelius, Cathleen and Kreiman, Jody},
	year = {1989},
	month = {01},
	pages = {207-226},
	title = {Recognition of emotional‐prosodic meanings in speech by autistic, schizophrenic, and normal children},
	volume = {5},
	journal = {Developmental Neuropsychology - DEVELOP NEUROPSYCHOL},
	doi = {10.1080/87565648909540433}
}
@article{bjorn1,
	author = {Schuller, Björn},
	year = {2018},
	month = {04},
	pages = {90-99},
	title = {Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends},
	volume = {61},
	journal = {Communications of the ACM},
	doi = {10.1145/3129340}
}
@unknown{meld,
	author = {Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada},
	year = {2018},
	month = {10},
	pages = {},
	title = {MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations}
}
@article{koolagudi,
	author = {Koolagudi, Shashidhar},
	year = {2012},
	month = {06},
	pages = {},
	title = {Emotion recognition from speech: A review},
	volume = {15},
	journal = {International Journal of Speech Technology},
	doi = {10.1007/s10772-011-9125-1}
}
@article{spnorm,
	author = {Schuller, Björn and Vlasenko, Bogdan and Eyben, Florian and Wöllmer, Martin and Stuhlsatz, Andre and Wendemuth, Andreas and Rigoll, Gerhard},
	year = {2010},
	month = {07},
	pages = {119-131},
	title = {Cross-Corpus Acoustic Emotion Recognition: Variances and Strategies},
	volume = {1},
	journal = {IEEE Transactions on Affective Computing},
	doi = {10.1109/T-AFFC.2010.8}
}
@article{hcf2,
	author = {Nwe, Tin and Foo, S.W. and De Silva, Liyanage},
	year = {2003},
	month = {11},
	pages = {603-623},
	title = {Speech Emotion Recognition Using Hidden Markov Models},
	volume = {41},
	journal = {Speech Communication},
	doi = {10.1016/S0167-6393(03)00099-2}
}
@inproceedings{hcf3,
	title={Issues in emotion-oriented computing – towards a shared understanding},
	author={Marc Schr{\"o}der and Roddy Cowie},
	year={2006}
}
@article{graves,
	author = {Graves, A. and Jaitly, Navdeep},
	year = {2014},
	month = {01},
	pages = {1764-1772},
	title = {Towards end-to-end speech recognition with recurrent neural networks},
	volume = {5},
	journal = {31st International Conference on Machine Learning, ICML 2014}
}
@article{tzir,
	author = {Tzirakis, Panagiotis and Trigeorgis, George and Nicolaou, Mihalis and Schuller, Björn and Zafeiriou, Stefanos},
	year = {2017},
	month = {04},
	pages = {},
	title = {End-to-End Multimodal Emotion Recognition Using Deep Neural Networks},
	volume = {PP},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	doi = {10.1109/JSTSP.2017.2764438}
}
@inproceedings{zhang,
	author = {Zhang, Zixing and Wu, Bingwen and Schuller, Björn},
	year = {2019},
	month = {05},
	pages = {6705-6709},
	title = {Attention-augmented End-to-end Multi-task Learning for Emotion Prediction from Speech},
	doi = {10.1109/ICASSP.2019.8682896}
}
@inproceedings{yuan,
	title={Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning},
	author={Yuanchao Li and Tianyu Zhao and Tatsuya Kawahara},
	booktitle={INTERSPEECH},
	year={2019}
}
@inproceedings{adieu,
	author = {Trigeorgis, George and Ringeval, Fabien and Brueckner, Raymond and Marchi, Erik and Nicolaou, Mihalis and Schuller, Björn and Zafeiriou, Stefanos},
	year = {2016},
	month = {03},
	pages = {5200-5204},
	title = {Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network},
	doi = {10.1109/ICASSP.2016.7472669}
}
@inproceedings{e2e,  author={P. {Tzirakis} and J. {Zhang} and B. W. {Schuller}},  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={End-to-End Speech Emotion Recognition Using Deep Neural Networks},   year={2018},  volume={},  number={},  pages={5089-5093},}
@article{mehmet,
	author = {Akçay, Berkehan and Oguz, Kaya},
	year = {2020},
	month = {01},
	pages = {},
	title = {Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers},
	volume = {116},
	journal = {Speech Communication},
	doi = {10.1016/j.specom.2019.12.001}
}
@INPROCEEDINGS{misramadi,  author={S. {Mirsamadi} and E. {Barsoum} and C. {Zhang}},  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Automatic speech emotion recognition using recurrent neural networks with local attention},   year={2017},  volume={},  number={},  pages={2227-2231},}
@article{dae,
	author = {Chao, Linlin and Tao, Jianhua and Yang, Minghao and Li, Ya},
	year = {2014},
	month = {10},
	pages = {341-344},
	title = {Improving generation performance of speech emotion recognition by denoising autoencoders},
	journal = {Proceedings of the 9th International Symposium on Chinese Spoken Language Processing, ISCSLP 2014},
	doi = {10.1109/ISCSLP.2014.6936627}
}
@article{adae,
	author = {Deng, Jun and Zhang, Zixing and Eyben, Florian and Schuller, Björn},
	year = {2014},
	month = {09},
	pages = {1068-1072},
	title = {Autoencoder-based Unsupervised Domain Adaptation for Speech Emotion Recognition},
	volume = {21},
	journal = {Signal Processing Letters, IEEE},
	doi = {10.1109/LSP.2014.2324759}
}
@INPROCEEDINGS{sdae,  author={J. {Deng} and Z. {Zhang} and E. {Marchi} and B. {Schuller}},  booktitle={2013 Humaine Association Conference on Affective Computing and Intelligent Interaction},   title={Sparse Autoencoder-Based Feature Transfer Learning for Speech Emotion Recognition},   year={2013},  volume={},  number={},  pages={511-516},}
@inproceedings{multi-domain,
	author = {Milner, Rosanna and Jalal, Md Asif and Ng, Raymond W. M. and Hain, Thomas},
	year = {2019},
	month = {12},
	pages = {},
	title = {A Cross-Corpus Study on Speech Emotion Recognition},
	doi = {10.1109/ASRU46091.2019.9003838}
}
@inproceedings{covarep,
	author = {Degottex, Gilles and Kane, John and Drugman, Thomas and Raitio, Tuomo and Scherer, Stefan},
	year = {2014},
	month = {05},
	pages = {},
	title = {COVAREP: A Collaborative Voice Analysis Repository for Speech Technologies},
	journal = {ICASSP},
	doi = {10.1109/ICASSP.2014.6853739}
}
@inbook{rnn1,
	author = {Fonnegra, Rubén and Díaz, Gloria},
	year = {2018},
	month = {01},
	pages = {882-892},
	title = {Speech Emotion Recognition Based on a Recurrent Neural Network Classification Model},
	isbn = {978-3-319-76269-2},
	doi = {10.1007/978-3-319-76270-8_59}
}
@inproceedings{rnn2,
	author = {Lee, Jinkyu and Tashev, Ivan},
	year = {2015},
	month = {09},
	pages = {},
	title = {High-level Feature Representation using Recurrent Neural Network for Speech Emotion Recognition}
}
@article{nnspic,
	author = {Bre, Facundo and Gimenez, Juan and Fachinotti, Víctor},
	year = {2017},
	month = {11},
	pages = {},
	title = {Prediction of wind pressure coefficients on building surfaces using Artificial Neural Networks},
	volume = {158},
	journal = {Energy and Buildings},
	doi = {10.1016/j.enbuild.2017.11.045}
}
@manual{stft6,
title  = {Playing with Discrete Fourier Transform Algorithm in JavaScript},
author = {Oleksii Trekhleb},
url    = {https://dev.to/trekhleb/playing-with-discrete-fourier-transform-algorithm-in-javascript-53n5},
year   = {2018}
}
@incollection{stft2,
	title = {CHAPTER 7 - Frequency Domain Processing},
	editor = {Nasser Kehtarnavaz},
	booktitle = {Digital Signal Processing System Design (Second Edition)},
	publisher = {Academic Press},
	edition = {Second Edition},
	address = {Burlington},
	pages = {175 - 196},
	year = {2008},
	isbn = {978-0-12-374490-6},
	doi = {https://doi.org/10.1016/B978-0-12-374490-6.00007-6},
	url = {http://www.sciencedirect.com/science/article/pii/B9780123744906000076},
	author = {Nasser Kehtarnavaz}
}
@inproceedings{mcs,
	author = {Kerkeni, Leila and Serrestou, Youssef and Mbarki, Mohamed and Raoof, Kosai and Mahjoub, Mohamed},
	year = {2018},
	month = {01},
	pages = {175-182},
	title = {Speech Emotion Recognition: Methods and Cases Study},
	doi = {10.5220/0006611601750182}
}
@inproceedings{librosa,
author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel and Mcvicar, Matt and Battenberg, Eric and Nieto, Oriol},
year = {2015},
month = {01},
pages = {18-24},
title = {librosa: Audio and Music Signal Analysis in Python},
url = {https://librosa.github.io/librosa/},
doi = {10.25080/Majora-7b98e3ed-003}
}
@article{batch_norm,
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	month = {02},
	pages = {},
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}
}
@article{leCun,  
author={Y. {LeCun} and B. {Boser} and J. S. {Denker} and D. {Henderson} and R. E. {Howard} and W. {Hubbard} and L. D. {Jackel}},  
journal={Neural Computation},   
title={Backpropagation Applied to Handwritten Zip Code Recognition},   
year={1989},  
volume={1},  
number={4},  
pages={541-551}
}
@manual{cnn_photo,
title  = {MNIST Handwritten Digits Classification using a Convolutional Neural Network (CNN)},
author = {Krut Patel},
url   ={https://towardsdatascience.com/mnist-handwritten-digits-classification-using-a-convolutional-neural-network-cnn-af5fafbc35e9},
year = {2019}
}
@manual{standord_cnn,
title  = {CS231n: Convolutional Neural Networks for Visual Recognition},
author = {Fei-Fei Li and Ranjay Krishna and Danfei Xu and Amelie Byun},
url   ={https://cs231n.github.io/convolutional-networks/},
year = {2020}
}@book{dpb,
title={Deep Learning},
author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
publisher={MIT Press},
note={\url{http://www.deeplearningbook.org}},
year={2016}
}
@book{hands,
	author = {Gron, Aurlien},
	title = {Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems},
	year = {2017},
	isbn = {1491962291},
	publisher = {O’Reilly Media, Inc.},
	edition = {1st}
}
@article{attention1,
	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
	year = {2014},
	month = {06},
	pages = {},
	title = {Recurrent Models of Visual Attention},
	volume = {3},
	journal = {Advances in Neural Information Processing Systems}
}
@misc{tensorflow,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
Mart\'{\i}n~Abadi and
Ashish~Agarwal and
Paul~Barham and
Eugene~Brevdo and
Zhifeng~Chen and
Craig~Citro and
Greg~S.~Corrado and
Andy~Davis and
Jeffrey~Dean and
Matthieu~Devin and
Sanjay~Ghemawat and
Ian~Goodfellow and
Andrew~Harp and
Geoffrey~Irving and
Michael~Isard and
Yangqing Jia and
Rafal~Jozefowicz and
Lukasz~Kaiser and
Manjunath~Kudlur and
Josh~Levenberg and
Dandelion~Man\'{e} and
Rajat~Monga and
Sherry~Moore and
Derek~Murray and
Chris~Olah and
Mike~Schuster and
Jonathon~Shlens and
Benoit~Steiner and
Ilya~Sutskever and
Kunal~Talwar and
Paul~Tucker and
Vincent~Vanhoucke and
Vijay~Vasudevan and
Fernanda~Vi\'{e}gas and
Oriol~Vinyals and
Pete~Warden and
Martin~Wattenberg and
Martin~Wicke and
Yuan~Yu and
Xiaoqiang~Zheng},
year={2015},
}
@inproceedings{emodb,
	title={F0-CONTOURS IN EMOTIONAL SPEECH},
	author={Astrid Paeschke and Miriam Kienast and Walter F. Sendlmeier},
	year={1999}
}
@article{ravdess,
	author = {Livingstone, Steven R. AND Russo, Frank A.},
	journal = {PLOS ONE},
	publisher = {Public Library of Science},
	title = {The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
	year = {2018},
	month = {05},
	volume = {13},
	url = {https://doi.org/10.1371/journal.pone.0196391},
	pages = {1-35},
	abstract = {The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.},
	number = {5},
	doi = {10.1371/journal.pone.0196391}
}
@inproceedings{emovo,
	author = {Costantini, Giovanni and Iadarola, Iacopo and Paoloni, and Todisco, Massimiliano},
	year = {2014},
	month = {05},
	pages = {},
	title = {EMOVO Corpus: an Italian Emotional Speech Database},
	isbn = {9782951740884}
}
@article{mav,
	author = {Belin, Pascal and Fillion-Bilodeau, Sarah and Gosselin, Frédéric},
	year = {2008},
	month = {05},
	pages = {531-9},
	title = {The Montreal Affective Voices: A validated set of nonverbal affect bursts for research on auditory affective processing},
	volume = {40},
	journal = {Behavior research methods},
	doi = {10.3758/BRM.40.2.531}
}
@inproceedings{enterface,
author = {Martin, O. and Kotsia, I. and Macq, Benoit and Pitas, I.},
year = {2006},
month = {02},
pages = {8 - 8},
title = {The eNTERFACE05 Audio-Visual Emotion Database},
isbn = {0-7695-2571-7},
doi = {10.1109/ICDEW.2006.145}
}
@manual{JL,
title  = {JL corpus},
author = {Li Tian},
url   ={https://www.kaggle.com/tli725/jl-corpus},
year = {2018}
}

@article{iemocap,
	author = {Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower Provost, Emily and Kim, Samuel and Chang, Jeannette and Lee, Sungbok and Narayanan, Shrikanth},
	year = {2008},
	month = {12},
	pages = {335-359},
	title = {IEMOCAP: Interactive emotional dyadic motion capture database},
	volume = {42},
	journal = {Language Resources and Evaluation},
	doi = {10.1007/s10579-008-9076-6}
}

@article{adam,
	author = {Kingma, Diederik and Ba, Jimmy},
	year = {2014},
	month = {12},
	pages = {},
	title = {Adam: A Method for Stochastic Optimization},
	journal = {International Conference on Learning Representations}
}
@inproceedings{compar1,
	author = {Kerkeni, Leila and Serrestou, Youssef and Mbarki, Mohamed and Raoof, Kosai and Mahjoub, Mohamed},
	year = {2018},
	month = {01},
	pages = {175-182},
	title = {Speech Emotion Recognition: Methods and Cases Study},
	doi = {10.5220/0006611601750182}
}
@inbook{compar2,
	author = {Fonnegra, Rubén and Díaz, Gloria},
	year = {2018},
	month = {01},
	pages = {882-892},
	title = {Speech Emotion Recognition Based on a Recurrent Neural Network Classification Model},
	isbn = {978-3-319-76269-2},
	doi = {10.1007/978-3-319-76270-8_59}
}
@inproceedings{compar3,
	author = {Lim, Wootaek and Jang, Daeyoung and Lee, Taejin},
	year = {2016},
	month = {12},
	pages = {1-4},
	title = {Speech emotion recognition using convolutional and Recurrent Neural Networks},
	doi = {10.1109/APSIPA.2016.7820699}
}
@article{imbun1,
	author = {Deng, Jun and Zhang, Zixing and Eyben, Florian and Schuller, Björn},
	year = {2014},
	month = {09},
	pages = {1068-1072},
	title = {Autoencoder-based Unsupervised Domain Adaptation for Speech Emotion Recognition},
	volume = {21},
	journal = {Signal Processing Letters, IEEE},
	doi = {10.1109/LSP.2014.2324759}
}
@article{imbun3,
	author = {ul haq, Sana and Jackson, Philip},
	year = {2010},
	month = {01},
	pages = {},
	title = {Multimodal Emotion Recognition},
	journal = {Machine Audition: Principles, Algorithms and Systems},
	doi = {10.4018/978-1-61520-919-4.ch017}
}

@inproceedings{comp1,
	author = {Kerkeni, Leila and Serrestou, Youssef and Mbarki, Mohamed and Raoof, Kosai and Mahjoub, Mohamed},
	year = {2018},
	month = {01},
	pages = {175-182},
	title = {Speech Emotion Recognition: Methods and Cases Study},
	doi = {10.5220/0006611601750182}
}

@article{comp2,
	title = "Speech emotion recognition with deep convolutional neural networks",
	journal = "Biomedical Signal Processing and Control",
	volume = "59",
	pages = "101894",
	year = "2020",
	issn = "1746-8094",
	doi = "https://doi.org/10.1016/j.bspc.2020.101894",
	url = "http://www.sciencedirect.com/science/article/pii/S1746809420300501",
	author = "Dias Issa and M. [Fatih Demirci] and Adnan Yazici",
	keywords = "Speech emotion recognition, Deep learning, Signal processing",
	abstract = "The speech emotion recognition (or, classification) is one of the most challenging topics in data science. In this work, we introduce a new architecture, which extracts mel-frequency cepstral coefficients, chromagram, mel-scale spectrogram, Tonnetz representation, and spectral contrast features from sound files and uses them as inputs for the one-dimensional Convolutional Neural Network for the identification of emotions using samples from the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), Berlin (EMO-DB), and Interactive Emotional Dyadic Motion Capture (IEMOCAP) datasets. We utilize an incremental method for modifying our initial model in order to improve classification accuracy. All of the proposed models work directly with raw sound data without the need for conversion to visual representations, unlike some previous approaches. Based on experimental results, our best-performing model outperforms existing frameworks for RAVDESS and IEMOCAP, thus setting the new state-of-the-art. For the EMO-DB dataset, it outperforms all previous works except one but compares favorably with that one in terms of generality, simplicity, and applicability. Specifically, the proposed framework obtains 71.61% for RAVDESS with 8 classes, 86.1% for EMO-DB with 535 samples in 7 classes, 95.71% for EMO-DB with 520 samples in 7 classes, and 64.3% for IEMOCAP with 4 classes in speaker-independent audio classification tasks."
}
@article{comp3,
	author = {Zeng, Yuni and Mao, Hua and Peng, Dezhong and Yi, Zhang},
	year = {2017},
	month = {12},
	pages = {},
	title = {Spectrogram based multi-task audio classification},
	volume = {78},
	journal = {Multimedia Tools and Applications},
	doi = {10.1007/s11042-017-5539-3}
}
@inproceedings{comp4,
	author = {Popova, Anastasiya and Rassadin, Alexandr and Ponomarenko, Alexander},
	year = {2018},
	month = {01},
	pages = {117-124},
	title = {Emotion Recognition in Sound},
	volume = {736},
	isbn = {978-3-319-66603-7},
	journal = {Studies in Computational Intelligence},
	doi = {10.1007/978-3-319-66604-4_18}
}

@inproceedings{comp5,
author = {Latif, Siddique and Rana, Rajib and Younis, Shahzad and Qadir, Junaid and Epps, Julien},
year = {2018},
month = {09},
pages = {257-261},
title = {Transfer Learning for Improving Speech Emotion Classification Accuracy},
doi = {10.21437/Interspeech.2018-1625}
}

@article{comp6,
	author = {Schuller, Björn},
	year = {2011},
	month = {06},
	pages = {77-87},
	title = {Affective Speaker State Analysis in the Presence of Reverberation},
	volume = {14},
	journal = {International Journal of Speech Technology},
	doi = {10.1007/s10772-011-9090-8}
}

@article{comp7,
	title = "A new approach of audio emotion recognition",
	journal = "Expert Systems with Applications",
	volume = "41",
	number = "13",
	pages = "5858 - 5869",
	year = "2014",
	issn = "0957-4174",
	doi = "https://doi.org/10.1016/j.eswa.2014.03.026",
	url = "http://www.sciencedirect.com/science/article/pii/S0957417414001638",
	author = "Chien Shing Ooi and Kah Phooi Seng and Li-Minn Ang and Li Wern Chew",
	keywords = "Audio emotion recognition, RBF neural network, Prosodic features, MFCC feature",
	abstract = "A new architecture of intelligent audio emotion recognition is proposed in this paper. It fully utilizes both prosodic and spectral features in its design. It has two main paths in parallel and can recognize 6 emotions. Path 1 is designed based on intensive analysis of different prosodic features. Significant prosodic features are identified to differentiate emotions. Path 2 is designed based on research analysis on spectral features. Extraction of Mel-Frequency Cepstral Coefficient (MFCC) feature is then followed by Bi-directional Principle Component Analysis (BDPCA), Linear Discriminant Analysis (LDA) and Radial Basis Function (RBF) neural classification. This path has 3 parallel BDPCA+LDA+RBF sub-paths structure and each handles two emotions. Fusion modules are also proposed for weights assignment and decision making. The performance of the proposed architecture is evaluated on eNTERFACE’05 and RML databases. Simulation results and comparison have revealed good performance of the proposed recognizer."
}



